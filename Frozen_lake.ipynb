{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "# Ehsan Kouchaki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')\n",
    "env.reset()\n",
    "V = np.zeros(64)\n",
    "theta = 1e-8\n",
    "gama = 1\n",
    "pi_a_s = 1/4;\n",
    "while True:\n",
    "    delta = 0\n",
    "    for s in range(64):\n",
    "        v = V[s]\n",
    "        \n",
    "        tmp = 0        \n",
    "        for a in range(4):\n",
    "            for Probability, sprime, r, end_of_episode in env.env.P[s][a]:\n",
    "                tmp += pi_a_s * Probability * (r + gama * V[sprime])                \n",
    "        V[s] = tmp\n",
    "        delta = max(delta, np.abs(v - V[s]))\n",
    "        \n",
    "    if delta < theta: break\n",
    "V = np.reshape(V, (8,8))\n",
    "np.set_printoptions(precision=2)\n",
    "print(V)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(V)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(1 / (1 + np.exp(-1000000*V)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')\n",
    "env.reset()\n",
    "theta = 1e-8\n",
    "gama = 0.9\n",
    "\n",
    "# Initialization\n",
    "Pi = np.zeros(64)\n",
    "V = np.zeros(64)\n",
    "\n",
    "V0 = []\n",
    "V00 = []\n",
    "while True:\n",
    "    \n",
    "    # Policy Evaluation\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(64):\n",
    "            v = V[s]            \n",
    "            tmp = 0        \n",
    "            a = Pi[s]\n",
    "            for Probability, sprime, r, end_of_episode in env.env.P[s][a]:\n",
    "                tmp += Probability * (r + gama * V[sprime])                \n",
    "            V[s] = tmp\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "            if s == 0: V0.append(tmp)\n",
    "        if delta < theta: break\n",
    "    V00.append(V[0])\n",
    "    \n",
    "    # Policy Improvement\n",
    "    policy_stable = True\n",
    "    for s in range(64):\n",
    "        old_action = Pi[s]\n",
    "        vi = np.zeros((4,1))        \n",
    "        for a in range(4):\n",
    "            for Probability, sprime, r, end_of_episode in env.env.P[s][a]:\n",
    "                vi[a][0] += Probability * (r + gama * V[sprime])\n",
    "        Pi[s] = np.argmax(vi)\n",
    "        if old_action != Pi[s]:\n",
    "            policy_stable = False\n",
    "    if policy_stable: break\n",
    "Vstar = np.reshape(V, (8,8))\n",
    "Pistar = np.reshape(Pi, (8,8)).astype(int)        \n",
    "\n",
    "np.set_printoptions(precision=5)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"The state value matrix is: \\n\", Vstar, \"\\n\")\n",
    "print(\"The Optimum Policy matrix is: \\n\", Pistar, \"\\n\")\n",
    "\n",
    "# plot of the optimum policy ac a guidance matrix\n",
    "PiMask = np.zeros(64)    # ceating the mask matrix for assigning O to hole and target cells at the end just for plotting\n",
    "PiMask[19] = 4; PiMask[29] = 4; PiMask[35] = 4; PiMask[41] = 4; PiMask[42] = 4    # hole cells\n",
    "PiMask[46] = 4; PiMask[49] = 4; PiMask[52] = 4; PiMask[54] = 4; PiMask[59] = 4    # hole sells\n",
    "PiMask[63] = 4    # target cell\n",
    "string = \"LDRUO\"\n",
    "Piplot = Pistar + np.reshape(PiMask, (8,8)).astype(int)\n",
    "for i in range(8):\n",
    "    print(['{i:j}'.join(string[Piplot[i, j]]) for j in range(8)])\n",
    "\n",
    "# plot of changes of state value of the start cell\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(V00)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(V0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')\n",
    "env.reset()\n",
    "gama = 0.9\n",
    "policy = Pistar.flatten()\n",
    "V = dict()\n",
    "Returns = defaultdict(list)\n",
    "V0 = []\n",
    "\n",
    "for _ in range(500000):\n",
    "    \n",
    "    # Generating episode using Pistar\n",
    "    episode = []\n",
    "    done = True\n",
    "    while True:\n",
    "        if done: St, Rt, done    = env.reset(), 0, False\n",
    "        else: St, Rt, done, _ = env.step(At)\n",
    "        At = policy[St]\n",
    "        episode.append((St, Rt, done, At))\n",
    "        if done: break\n",
    "    \n",
    "    G = 0\n",
    "    for t in range(len(episode)-2,-1,-1):\n",
    "        St = episode[t][0]\n",
    "        Rt_p1 = episode[t+1][1]\n",
    "        G = gama * G + Rt_p1\n",
    "        \n",
    "        notseen = True\n",
    "        for i in range(t):\n",
    "            if St == episode[i][0]: notseen = False\n",
    "        if notseen:\n",
    "            Returns[St].append(G)\n",
    "            if len(Returns[St]) == 1: V[St] = np.average(Returns[St])\n",
    "            else: V[St] = V[St] + (G - V[St]) / len(Returns[St])                  \n",
    "            if St == 0: V0.append(V[St])\n",
    "            \n",
    "plt.plot(V0)\n",
    "Vi = np.zeros((64,1))\n",
    "for i in V: Vi[i] = V[i]\n",
    "print(\"The state value matrix is: \\n\\n\", Vi.reshape(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')\n",
    "env.reset()\n",
    "gama = 0.9\n",
    "\n",
    "# initialize\n",
    "Q = np.zeros((64 , 4))\n",
    "C = np.zeros((64 , 4))\n",
    "Pi = np.zeros((64 , 1))\n",
    "\n",
    "for _ in range(1000000):\n",
    "    \n",
    "    # Grnerating episode using b\n",
    "    episode = []\n",
    "    done = True\n",
    "    while True:\n",
    "        if done: St, Rt, done    = env.reset(), 0, False\n",
    "        else: St, Rt, done, _ = env.step(At)\n",
    "        At = np.random.randint(4)    # b = np.random.randint(4)\n",
    "        episode.append((St, Rt, done, At))\n",
    "        if done: break\n",
    "    \n",
    "    G = 0\n",
    "    W = 1\n",
    "    \n",
    "    for t in range(len(episode)-2,-1,-1):\n",
    "        St = episode[t][0]\n",
    "        Rt_p1 = episode[t+1][1]\n",
    "        At = episode[t][3]\n",
    "        G = gama * G + Rt_p1\n",
    "        C[St][At] += W\n",
    "        Q[St][At] += (W / C[St][At]) * (G - Q[St][At])\n",
    "        Pi[St] = np.argmax(Q[St])\n",
    "        if At != Pi[St]: break\n",
    "        W = W / 0.25    # b_At_St = 0.25\n",
    "\n",
    "print(Pi.reshape(8,8))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "18087f5bfb713a2e83db214d55090eb4e1f5555a66d44884a268f6d0a872b880"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
